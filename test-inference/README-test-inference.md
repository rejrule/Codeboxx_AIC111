# test-inference â€” DÃ©ploiement dâ€™un modÃ¨le PyTorch avec AWS SAM (AIC111)

Ce dossier contient une **API serverless** qui reÃ§oit un nombre `x` et retourne la prÃ©diction `y` dâ€™une fonction approximÃ©e (ici un petit modÃ¨le PyTorch Ã©quivalent Ã  **y = 3x + 1**).  
Lâ€™API est packagÃ©e en **image Docker** et dÃ©ployÃ©e sur **AWS Lambda** derriÃ¨re **API Gateway**.

---

## âœ… URL publique (dÃ©jÃ  dÃ©ployÃ©e)

> **POST** `https://seyq4lqbog.execute-api.us-east-1.amazonaws.com/Prod/classify_digit/`  
> **Headers**: `Content-Type: application/json`  
> **Body**: `{"x": 2}`  
> **RÃ©ponse attendue**: `{"prediction": 7.0}`

### Tester rapidement (curl)
```bash
curl -s -X POST "https://seyq4lqbog.execute-api.us-east-1.amazonaws.com/Prod/classify_digit/" \
  -H "Content-Type: application/json" \
  -d '{"x": 2}'
# {"prediction": 7.0}
```

> â„¹ï¸ **Notes**  
> - La premiÃ¨re requÃªte peut prendre quelques secondes (**cold start**). Si vous voyez `{"message": "Endpoint request timed out"}`, relancez la mÃªme requÃªte et elle devrait rÃ©ussir.  
> - Le **timeout API Gateway** est 30 s.

---

## ğŸ“ Structure utile du projet

```
test-inference/
â”œâ”€ app/
â”‚  â”œâ”€ app.py               # Code de la Lambda (handler: app.lambda_handler)
â”‚  â””â”€ requirements.txt     # DÃ©pendances (CPU-only) : torch==2.2.2
â”œâ”€ events/
â”‚  â””â”€ ping.json            # Ã‰vÃ©nement dâ€™exemple pour sam local invoke
â”œâ”€ template.yaml           # DÃ©finition SAM (Lambda + API Gateway)
â””â”€ README.md               # Ce fichier (Ã  placer ici)
```

- **app/app.py** : charge un mini-modÃ¨le PyTorch (ou valeurs par dÃ©faut) et retourne `{"prediction": <float>}`.  
- **app/requirements.txt** : dÃ©pendances minimalistes (PyTorch CPU) pour Ã©viter les conflits.  
- **template.yaml** :  
  - Chemin API: **POST** `/classify_digit`  
  - **MemorySize**: 512 MB  
  - **PackageType**: Image (base `public.ecr.aws/lambda/python:3.9`)  
  - Pas dâ€™authentification (API publique pour besoins du cours).  

---

## ğŸ§ª Tests **locaux** (Docker + SAM)

### PrÃ©requis
- **Docker Desktop** en marche
- **AWS SAM CLI**
- (facultatif) Python 3.x

### Lancer en local
```bash
cd test-inference

# Build de lâ€™image Lambda via Docker
sam build --use-container

# DÃ©marrer lâ€™API locale (port 3000)
sam local start-api
```

Dans un autre terminal, appelez lâ€™API locale :
```bash
curl -s -X POST http://127.0.0.1:3000/classify_digit \
  -H "Content-Type: application/json" \
  -d '{"x": 2}'
# {"prediction": 7.0}
```

### Invocation locale directe (Ã©vÃ©nement API Gateway)
```bash
# Exemple dâ€™event (dÃ©jÃ  fourni dans events/ping.json):
# {
#   "resource": "/classify_digit",
#   "path": "/classify_digit",
#   "httpMethod": "POST",
#   "headers": { "Content-Type": "application/json" },
#   "isBase64Encoded": false,
#   "body": "{\"x\": 2}"
# }

sam local invoke InferenceFunction -e events/ping.json
# ... "body":"{\"prediction\": 7.0}"
```

---

## ğŸš€ (Re)dÃ©ploiement sur AWS (propriÃ©taire du compte)

> âš ï¸ Non requis pour le coach qui **teste** seulement lâ€™URL publique.  
> Ces commandes sont pour le propriÃ©taire du compte AWS.

PrÃ©requis :  
- **AWS CLI** configurÃ© (`aws configure`) ou `AWS_PROFILE=sam-deployer`
- **Docker** + **SAM CLI**

```bash
cd test-inference

# Build image (Docker)
sam build --use-container

# Premier dÃ©ploiement guidÃ© (rÃ©pondre aux invites)
sam deploy --guided
# RÃ©gion: us-east-1
# Confirmer changements: Y
# Autoriser crÃ©ation de rÃ´les IAM: Y
# DÃ©sactiver rollback: N
# Pas dâ€™auth pour InferenceFunction: Y
# Sauver la config: Y

# DÃ©ploiements suivants
sam deploy
```

En fin de dÃ©ploiement, SAM affiche les **Outputs**, notamment :  
- `InferenceApi` â†’ lâ€™URL publique (`.../Prod/classify_digit/`)  
- `InferenceFunction` â†’ ARN de la Lambda  
- `InferenceFunctionIamRole` â†’ RÃ´le IAM

---

## ğŸ” DÃ©pannage

- **Timeout Ã  la premiÃ¨re requÃªte** : relancer la requÃªte (cold start).  
- **Logs CloudWatch (dÃ©ployÃ©)** :
```bash
sam logs -n InferenceFunction --stack-name "test-inference" --tail
```
- **Tester localement sans API** :
```bash
sam local invoke InferenceFunction -e events/ping.json
```
- **Docker** doit Ãªtre **running** pour `sam build --use-container` et `sam local start-api`.

---

## ğŸ§¹ Nettoyage (propriÃ©taire du compte AWS)

Pour supprimer toutes les ressources AWS crÃ©Ã©es par cette stack :
```bash
sam delete --stack-name "test-inference"
```

---

## ğŸ“¦ DÃ©tails techniques rapides

- **Runtime** : Python 3.9 (image `public.ecr.aws/lambda/python:3.9`)  
- **Inference** : PyTorch CPU (`torch==2.2.2`)  
- **MÃ©moire Lambda** : 512 MB  
- **Endpoint** : `POST /classify_digit`  
- **Corps attendu** : `{"x": <nombre>}`  
- **RÃ©ponse** : `{"prediction": <nombre>}`

---

## ğŸ‘¤ Livrables (pour le cours)

Inclure dans votre livrable :
- Votre **nom complet**  
- **Lien vers vos 5 vidÃ©os** (format `mod11_video<n>.mp4` ou `.avi`)  
- **URL de lâ€™API** : `https://seyq4lqbog.execute-api.us-east-1.amazonaws.com/Prod/classify_digit/`

---

Bon test !
